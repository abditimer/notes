{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipelines\n",
    "\n",
    "This document will take you through understanding what a pipeline is in Azure Machine Learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "### But i've already heard of pipelines?\n",
    "\n",
    "In AML, we run a process as an experiment. This experiment can pull from data stores and use azure compute.\n",
    "\n",
    "In a traditional ML project, you normally split your overall process into tasks that complete the data wrangling, modeling and visualisation/deployment separately. These can be split into smaller tasks and orchestrated together to form a `pipeline`.\n",
    "\n",
    "Pipelines are a common term in ML. You will come across it in the following areas:\n",
    "- scikit learn pipeline: combine data preprocessing transformations and ML modeling\n",
    "- azure devops pipeline: these pipelines are for defining your build or release process \n",
    "- azure machine learning pipelines: group steps needed to run tasks in AML\n",
    "\n",
    "Hence, you could have a scikit learn pipeline, running in an AML pipeline, that is initiated by a devops pipeline.\n",
    "\n",
    "### What are pipelines?\n",
    "A pipeline in AML refers to a `workflow` where each task within the workflow is refered to as a `step`. These steps can run in a sequence, or parallel. \n",
    "\n",
    "#### Steps of a ML pipeline\n",
    "Step types:\n",
    "- PythonScriptStep: runs a python script\n",
    "- EstimatorStep: runs an estimator \n",
    "- DataTransferStep: Use ADF to copy data between data stores \n",
    "- DatabricksStep: Runs a notebook/script/JAR on databricks cluster \n",
    "- AdlaStep: runs U-SQL job in Azure Data Lake Analytics\n",
    "\n",
    "### Example - lets create a pipeline.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.pipeline.steps import PythonScriptStep, EstimatorStep\n",
    "from azureml.pipeline.core import pipeline\n",
    "from azureml.core import Experiment\n",
    "\n",
    "# construct experiment\n",
    "experiment = Experiment(workspace=ws, name='training-pipeline')\n",
    "\n",
    "# step1: run python script\n",
    "step1 = PythonScriptStep(\n",
    "    name='prepare data',\n",
    "    source_directory='scripts',\n",
    "    script_name='data_prep.py',\n",
    "    compute_target='aml-cluster',\n",
    "    runconfig=run_config\n",
    ")\n",
    "\n",
    "# step2: run estimator\n",
    "step2 = EstimatorStep(\n",
    "    name='train model',\n",
    "    estimator= sk_estimator,\n",
    "    compute_target='aml-cluster'\n",
    ")\n",
    "\n",
    "# construct pipeline\n",
    "train_pipeline= Pipeline(workspace=ws, steps=[step1, step2])\n",
    "\n",
    "# run pipeline\n",
    "pipeline_run = experiment.submit(train_pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What if we want to pass data between pipeline steps?\n",
    "\n",
    "We use a `PipelineData object`:\n",
    "- references a location in a datastore\n",
    "- creates a data dependency between pipeline steps. It is a place to store data whilst between pipeline steps\n",
    "\n",
    "To use a PipelineData object to pass data between two steps in a pipeline, you have to:\n",
    "- Define PipelineData Object that references a location in a datastore\n",
    "- It must be an input or an output for the steps that use It\n",
    "- If it is an input, provide PipelineData object as a script parameter for the scripts that use it\n",
    "\n",
    "\n",
    "For example:\n",
    "If you had a python script that was called:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### This is the python script\n",
    "%%writefile somefolder/somescript.py\n",
    "from azureml.core import Run\n",
    "from argparse\n",
    "import os\n",
    "\n",
    "# get experiment run context\n",
    "run = Run.get_context()\n",
    "\n",
    "# get dataset that is input into df\n",
    "raw_df = run.input_datasets['raw_data'].to_pandas_dataframe()\n",
    "\n",
    "# get pipeline arguments\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\n",
    "    '--folder', type=str, dest='folder'\n",
    ")\n",
    "args = parser.parse_args()\n",
    "output_folder= args.folder\n",
    "\n",
    "# code to prep data\n",
    "prepped_df = raw_df[['col1', 'col2', 'col3']]\n",
    "\n",
    "# save prepped data to the PipelineData location\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "output_path = os.path.join(output_folder, 'prepped_data.csv')\n",
    "prepped_df.to_csv(output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.pipeline.core import PipelineData\n",
    "from azureml.pipeline.steps import PythonScriptStep, EstimatorStep\n",
    "\n",
    "# get dataset for the initial data\n",
    "raw_ds = Dataset.get_by_name(ws, 'raw_dataset')\n",
    "\n",
    "# get default datastore\n",
    "data_store = ws.get_default_datastore()\n",
    "\n",
    "# define PipelineData object to pass between pipeline steps\n",
    "prepped_data = PipelineData('prepped', datastore=data_store)\n",
    "\n",
    "# pipeline step1: python script\n",
    "step1 = PythonScriptStep(\n",
    "    name='prepped data',\n",
    "    source_directory='scripts',\n",
    "    compute_target='aml-cluster',\n",
    "    runconfig=run_config,\n",
    "    # input dataset\n",
    "    inputs=[\n",
    "        raw_ds.as_named_input('raw_data')\n",
    "    ],\n",
    "    # pipelinedata obj for output\n",
    "    outputs = [\n",
    "        prepped_data\n",
    "    ],\n",
    "    arguments=['--folder', prepped_data]\n",
    ")\n",
    "\n",
    "# pipeline step2: estimator\n",
    "step2 = EstimatorStep(\n",
    "    name='train model',\n",
    "    estimator=sk_estimator,\n",
    "    compute_target='aml-cluster',\n",
    "    inputs=[prepped_data],\n",
    "    estimator_entry_script_arguments=['--folder', prepped_data]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reusing pipeline steps\n",
    "\n",
    "AML pipeline has a caching and reuse feature for pipelines that are long-running and have multiple steps.\n",
    "\n",
    "### Managing the multi step output reuse\n",
    "Default: previous pipeline output is reused\n",
    "- However, this changes if the script, source directory or if the parameters change\n",
    "\n",
    "This can reduce time, but can lead to stale results when changes to later data sources are not accounted for.\n",
    "\n",
    "To control reuse, specify the `allow_reuse` parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stepX = PythonScriptStep(\n",
    "    name='prep data',\n",
    "    source_directory='scripts',\n",
    "    script_name='data_prep.py',\n",
    "    compute_target='aml-cluster',\n",
    "    runconfig=run_config,\n",
    "    inputs=[raw_ds.as_named_input('raw_data')],\n",
    "    outputs=[prepped_data],\n",
    "    arguments=['--folder', prepped_data],\n",
    "    # distable step reuse - forcing reloading this step\n",
    "    allow_reuse = False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also force the entire pipeline to rerun by setting the `regenerate_outputs` param:\n",
    "```python\n",
    "pipeline_run = experiment.submit(train_pipeline, regenerate_outputs=True)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Publishing pipelines\n",
    "\n",
    "How can we publish a pipeline, so that others can use our amazing code?\n",
    "\n",
    "The answer is by creating a REST endpoint. This will allow the pipeline to be ran on demand. \n",
    "\n",
    "There are two ways to do this:\n",
    "- call the `publish` method of the pipeline before the run\n",
    "- call the `publish` method on a successfuly run pipeline\n",
    "\n",
    "```python\n",
    "published_pipeline=pipeline.publish(\n",
    "    name='training-pipline',\n",
    "    description='model training pipeline',\n",
    "    version='1.0'\n",
    ")\n",
    "```\n",
    "\n",
    "***Or***\n",
    "\n",
    "```python\n",
    "published_pipeline=run.publish_pipeline(\n",
    "    name='training-pipeline',\n",
    "    description='model training pipeline',\n",
    "    version='1.0'\n",
    ")\n",
    "```\n",
    "\n",
    "You can check if you have successfuly published a pipeline via:\n",
    "- AML studio portal\n",
    "- check the URI endpoint\n",
    "\n",
    "```python\n",
    "rest_endpoint = published_pipeline.endpoint\n",
    "print(rest_endpoint)\n",
    "```\n",
    "\n",
    "But how can I then interact and call this REST endpoint? Well sir, that is the easy part.\n",
    "\n",
    "you just have to make a HTTP request to the endpoint, passing :\n",
    "- authorisation header: with a token for a service principle with permission to run the pipeline\n",
    "- JSON payload: specifying the experiment name\n",
    "\n",
    "Example:\n",
    "```python\n",
    "import requests\n",
    "\n",
    "response = requests.post(\n",
    "    rest_endpoint,\n",
    "    headers=auth_header,\n",
    "    json = {\n",
    "        'ExperimentName': 'run_training_pipeline'\n",
    "    }\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What about passing parameters to a pipeline?\n",
    "\n",
    "You can use a `PipelineParameter` object to pass parameters to a pipeline.\n",
    "\n",
    "Here is an example:\n",
    "```python\n",
    "from azureml.pipeline.core.graph import PipelineParameter\n",
    "\n",
    "reg_param = PipelineParameter(name='reg_rate',default_value=0.1)\n",
    "\n",
    "# some more code\n",
    "\n",
    "# pipeline step\n",
    "step2 = EstimatorStep(\n",
    "    name='train model',\n",
    "    estimator=sk_estimator,\n",
    "    compute_target='aml-cluster',\n",
    "    inputs=[prepped],\n",
    "    estimator_entry_script_arguments= [\n",
    "        '--folder', prepped,\n",
    "        '--reg',reg_param\n",
    "    ]\n",
    ")\n",
    "```\n",
    "\n",
    "You can also pass the parameters needed in a step through JSON payloads for pipelines that are published. \n",
    "\n",
    "Here is an example:\n",
    "```python\n",
    "response = request.post(\n",
    "    rest_endpoint,\n",
    "    headers=auth_header,\n",
    "    json={\n",
    "        'ExperimentName':'run_training_pipeline',\n",
    "        'ParameterAssignments': {\n",
    "            'reg_rate':0.1\n",
    "        }\n",
    "    }\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What about if I want to schedule my pipeline?\n",
    "\n",
    "Once you have published your pipeline, you can:\n",
    "- call it on demand\n",
    "- run it automatically based on a periodic schedule/based on response on data update\n",
    "\n",
    "### schedule for periodic intervals\n",
    "To create a Schedule, you must define a `ScheduleRecurrence` that determines the run frequence.\n",
    "\n",
    "Example:\n",
    "\n",
    "```python\n",
    "from azureml.pipeline.core import ScheduleRecurrence, Schedule\n",
    "\n",
    "daily = ScheduleRecurrence(\n",
    "    frequency='Day',\n",
    "    intervals=1\n",
    ")\n",
    "\n",
    "pipeline_schedule = Schedule.create(\n",
    "    ws,\n",
    "    name='Daily Training',\n",
    "    description='trains model every day',\n",
    "    pipeline_id = published_pipeline.id,\n",
    "    experiment_name='Training_Pipeline',\n",
    "    recurrence=daily\n",
    ")\n",
    "```\n",
    "\n",
    "### Schedule on data change\n",
    "What if we want the pipeline to run on data changing? Here, we have to create a Schedule that monitors a specific path on a datastore.\n",
    "\n",
    "```python\n",
    "from azureml.core import datastore\n",
    "from azureml.pipeline.core import Schedule\n",
    "\n",
    "training_datastore = Datastore(workspace=ws, name='blob-data')\n",
    "pipeline_schedule=Schedule.create(\n",
    "    ws,\n",
    "    name='Reactive Training',\n",
    "    description='trains model on data change',\n",
    "    pipeline_id=published_pipeline_id,\n",
    "    experiment_name='Training_Pipeline',\n",
    "    datastore=training_datastore,\n",
    "    path_on_datastore='data/training'\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}