{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tune Hyperparams with AML\n",
    "\n",
    "In ML, models are trained to predict unknown labels for new data based on correlations between known labels and features in the training data. Depending on the algo, you may need to specify hyperparams to configure how the model is trained. \n",
    "E.g. logist regression uses regularisation rate hyperparam to counteract overfitting. \n",
    "\n",
    "remember:\n",
    "- parameters are derived from the training data by the model\n",
    "- hyperparameters are not \n",
    "\n",
    "Hyperparameter tuning happens when you are able to train the same algorithm and training data, but different hyperparameter values. The output model is then evaluated against one another to check the performance metric for which you want to optimise, and then allowing you to select the best model.\n",
    "\n",
    "In AML, you can do this in an experiment type called **hyperdrive run**. This initiates a child run for each hyperparameter combination that needs to be tested. Each child run uses a training script with parameterised hyperparameter values that trains the model, and logs the target performance metric."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define a Search Space\n",
    "\n",
    "Combination of hyperparameter values tried during tuning is known as **search space**. \n",
    "\n",
    "You can specify your hyperparameters as:\n",
    "- a value\n",
    "- values:\n",
    "    - list - choice([10, 20, 30])\n",
    "    - range - choice(range([1,10]))\n",
    "    - comma-separated values - choice(30,50,100)\n",
    "- distribution:\n",
    "    - qnormal\n",
    "    - quniform\n",
    "    - qlognormal\n",
    "    - qloguniform\n",
    "\n",
    "To define a search space, you have to create a dictionary with the appropriate parameters. e.g.\n",
    "```python\n",
    "from azureml.train.hyperdrive import choice, normal\n",
    "\n",
    "param_space={\n",
    "    '--batch_size':choice(16,32,64),\n",
    "    '--learning_rate': normal(10,3)\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Types of sampling\n",
    "\n",
    "Types of sampling:\n",
    "- Grid\n",
    "- Random\n",
    "- Bayesian\n",
    "\n",
    "#### Grid Sampling \n",
    "This is only used when all hyperparams are discrete, and when you want to try all possible combinations. e.g.\n",
    "```python\n",
    "from azureml.train.hyperdrive import GridParameterSamping, choice\n",
    "param_space = {\n",
    "    '--batch_size': choice(16, 32, 64),\n",
    "    '--learning_rate': choice(0.01, 0.1, 1.0)\n",
    "}\n",
    "param_sampling = GridParameterSampling(param_space)\n",
    "```\n",
    "\n",
    "#### Random Sampling \n",
    "Used to randomly select values for each hyperparameter. This can be a mix of discrete and continuous values. e.g.\n",
    "```python\n",
    "from azureml.train.hyperdrive import RandomParameterSampling, choice, normal\n",
    "\n",
    "param_space = {\n",
    "    '--batch_size': choice(16,32,64),\n",
    "    '--learning_rate': normal(10,3)\n",
    "}\n",
    "```\n",
    "\n",
    "#### Bayesian Sampling \n",
    "choose hyperparam values based on Bayesian optimsation algorithms. This tries to select parameter combinations that will result in improved performance based on previous selections. You can only use choice, uniform or quniform params with bayesian sampling. You can also not use early-termination policy. e.g.\n",
    "```python \n",
    "param_space = {\n",
    "    '--batch_size': choice(16,32,64),\n",
    "    '--learning_rate': uniform(0.5, 0.1)\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Early Termination\n",
    "\n",
    "If you ahve a large hyperparam search space, it may take too long to try all possible combinations. Normally, you set a max number of iterations, but this can still lead to too many runs that dont result in better model.\n",
    "\n",
    "To stop wasting time, you can set an early termination policy. It will abandon runs that are unlikely to produce a better results than previous runs. \n",
    "\n",
    "It is evaluated at **evaluation_interval**, this is a parameter that you specify, based on each time the target perfomance metric is logged. You can also set ***delay_evaluation*** param to avoid evaluating policy until a minimum number of iterations have run.\n",
    "\n",
    "Early termination is particularly helpful for Deep Neural Nets, as they are trained iteratively over a number of epochs.\n",
    "\n",
    "Types of policies:\n",
    "- Bandit\n",
    "- Median Stopping\n",
    "- Truncation\n",
    "\n",
    "#### Bandit policy\n",
    "use this to stop a run if the target performance metric is underperforming the best run so far by a specified margin.\n",
    "```python\n",
    "from azureml.train.hyperdrive import BanditPolicy\n",
    "\n",
    "early_termination_policy = BanditPolicy(\n",
    "    slack_amount = 0.2, \n",
    "    evaluation_interval = 1,\n",
    "    delay_evaluation = 5\n",
    ")\n",
    "- delay_evaluation: when to start. In this case, it will run after the first five\n",
    "- slack_amount: when to abandon. It will abandon runs when the reported taret metric is 0.2 worse than the best performing run\n",
    "\n",
    "#### Median Stopping policy\n",
    "This abandons runs where the target performance metric is worse than the median of the running averages for all runs. \n",
    "```python\n",
    "from azureml.train.hyperdrive import MedianStoppingPolicy\n",
    "\n",
    "early_termination_policy = MedianStoppingPolicy(\n",
    "    evaluation_interval=1,\n",
    "    delay_evaluation=5\n",
    ")\n",
    "```\n",
    "\n",
    "#### Truncation Selection policy\n",
    "This cancels the lowest performing X% of runs at each evaluation interval based on the truncation_percentage value you specified.\n",
    "```python\n",
    "from azureml.train.hyperdrive import TruncationSelectionPolicy\n",
    "\n",
    "early_termination_policy = TruncationSelectionPolicy(\n",
    "    truncation_percentage=10,\n",
    "    evaluation_interval=1,\n",
    "    delay_evaluation=5\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running a hyperparam tuning experiment\n",
    "\n",
    "In AML, you can tune hyperparameters by running a ***hyperdrive*** experiment.\n",
    "\n",
    "To run a hyperdrive experiment, you **must create a training script** and:\n",
    "- include an argument for each hyperparameter you want to vary\n",
    "- log the target performance metric. This will allow the hyperdrive run to evaluate the child runs it initiaties, and identify the best one that produces the best performing model\n",
    "\n",
    "sample:\n",
    "\n",
    "```python\n",
    "#libraries go here \n",
    "\n",
    "# Get regularization hyperparameter\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--regularization', type=float, dest='reg_rate', default=0.01)\n",
    "args = parser.parse_args()\n",
    "reg = args.reg_rate\n",
    "\n",
    "# Get the experiment run context\n",
    "run = Run.get_context()\n",
    "\n",
    "# get and split data\n",
    "\n",
    "# Train a logistic regression model with the reg hyperparameter\n",
    "model = LogisticRegression(C=1/reg, solver=\"liblinear\").fit(X_train, y_train)\n",
    "\n",
    "# calculate and log accuracy\n",
    "y_hat = model.predict(X_test)\n",
    "acc = np.average(y_hat == y_test)\n",
    "run.log('Accuracy', np.float(acc))\n",
    "\n",
    "# Save the trained model\n",
    "os.makedirs('outputs', exist_ok=True)\n",
    "joblib.dump(value=model, filename='outputs/model.pkl')\n",
    "\n",
    "run.complete()\n",
    "\n",
    "```\n",
    "\n",
    "### Configuring and running a hyperdrive experiment\n",
    "\n",
    "To prepare a hyperdrive exp, you have to use a HyperDriveConfig object to configure the experiment.\n",
    "```python\n",
    "from azureml.core import Experiment \n",
    "from azureml.train.hyperdrive import HyperDriveConfig, PrimaryMetricGoal \n",
    "\n",
    "# assumes ws, sklearn_estimator and param_sampling are already defined\n",
    "hyperdrive = HyperDriveConfig(\n",
    "    estimator = sklearn_estimator, # above\n",
    "    hyperparameter_sampling = param_sampling, #above\n",
    "    policy = None, # can be from above \n",
    "    primary_metric_name= 'Accuracy',\n",
    "    primary_metric_goal = PrimaryMetricGoal.MAXIMIZE,\n",
    "    max_total_runs = 6,\n",
    "    max_concurrent_runs = 4\n",
    ")\n",
    "experiment = Experiment(ws, 'hyperdrive_training')\n",
    "hyperdrive_run = experiment.submit(config=hyperdrive)\n",
    "```\n",
    "\n",
    "### Monitoring & Reviewing run \n",
    "\n",
    "You can monitor an experiment the usual way:\n",
    "- In studio or using RunDetails(run) widget\n",
    "\n",
    "The experiment will create a child run for each hyperparam combination to be tried, and you can review the logged metrics by:\n",
    "```python\n",
    "for child_run in run.get_children():\n",
    "    print(child_run.id, child_run.get_metrics())\n",
    "```\n",
    "\n",
    "To retreive the best performing run, you can:\n",
    "```python\n",
    "best_run = hyperdrive_run.get_best_run_by_primary_metric()\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}