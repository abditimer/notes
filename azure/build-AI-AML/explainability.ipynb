{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explainability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Importance\n",
    "\n",
    "We can look to quantify how much influence a single feature has on a label prediction. Explainers work by evaluating a test data set of feature cases and the labels it predicts for them.\n",
    "\n",
    "### Globale Feature Importance\n",
    "\n",
    "Global feature importance quanitifies the importance of each feature in the test dataset as a whole. it provides a general comparison of the extent to which each feature influences the prediction.\n",
    "\n",
    "### Local Feature Importance\n",
    "\n",
    "Local Feature importance measures the influence of each feature value for a specific individual prediction.\n",
    "\n",
    "In a binary classification model, each feature gets a local importance value for each possible class, showing the amount of support for each class that it can predict. \n",
    "\n",
    "#### Mismatch between Global and Local features\n",
    "\n",
    "There could be multiple reasons why local importance for an individual prediction varies from global importance for the entire dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Explainers\n",
    "\n",
    "You can use AML SDK to create explainers for ML models, even if the model was not trained using an experiment.\n",
    "\n",
    "### Creating an explainer\n",
    "to interpret a locally created model, you must install the **azureml-interpret** package. You can use this package to create an explainer. \n",
    "\n",
    "There are multiple types of explainers:\n",
    "- MimicExplainer\n",
    "    - This is an explainer that creates a global surrogate model that approximates your trained model and can be used to generate explanations. This surrogate model must have the same architecture as your model (e.g. linear or tree based)\n",
    "-  TabularExplainer\n",
    "    - An explainer that acts as a wrapper around various SHAP explainer algorithms, automatically choosing the one that is most appropriate for your model architecture\n",
    "- PFIExplainer\n",
    "    - A permutation Feature Importance explainer that analyses feature importance by shuffling feature values and measuring the impact on prediction performance\n",
    "\n",
    "imagine we have a hypothetical model called `loan_model`, lets create the above explainer types:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explaining Global Feature Importance\n",
    "\n",
    "To retreive global importance values, you call **explain_global()** method of your explainer to get a global explanation, then you can use the **get_feature_importance_dict()** method to get a dictionary of feature importance values. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explaining local feature importance\n",
    "\n",
    "to generate local feature importance from a MimicExplainer or TabularExplainer, you must call **explain_local()** method.\n",
    "\n",
    "You can use **get_ranked_local_names()** and **get_ranked_local_values()** methods to retreive dictionaries of the feature names & values, ranked by importance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MimicExplainer\n",
    "from interpret.ext.blackbox import MimicExplainer\n",
    "from interpret.ext.glassbox import DecisionTreeExplainableModel\n",
    "\n",
    "mim_explainer = MimicExplainer(\n",
    "    model = loan_model,\n",
    "    initialization_examples = X_test,\n",
    "    explainable_model = DecisionTreeExplanableModel,\n",
    "    features = ['loan_amount','income','age','marital_status'],\n",
    "    classes = ['reject', 'approve']\n",
    ")\n",
    "# global feature importance\n",
    "global_mim_explainer = mim_explainer.explain_global(X_test)\n",
    "global_mim_feature_importance = global_mim_explanation.get_feature_importance_dict()\n",
    "# local feature importance\n",
    "local_mim_explanation = mim_explainer.explain_local(X_test[0:5])\n",
    "local_mim_features = local_mim_explanation.get_ranked_local_names()\n",
    "local_mim_importance = local_mim_explanation.get_ranked_local_values()\n",
    "\n",
    "\n",
    "\n",
    "# TabularExplainer\n",
    "from interpret.ext.blackbox import TabularExplainer \n",
    "\n",
    "tab_explainer = TabularExplainer(\n",
    "    model = loan_model,\n",
    "    initialization_examples = X_test,\n",
    "    features = ['loan_amount','income','age','marital_status'],\n",
    "    classes = ['reject', 'approve']\n",
    ")\n",
    "# global feature importance\n",
    "global_tab_explanation = tab_explainer.explain_global(X_train)\n",
    "global_tab_feature_importance = global_tab_explanation.get_feature_importance_dict()\n",
    "# local feature importance\n",
    "local_tab_explanation = tab_explainer.explain_local(X_test[0:5])\n",
    "local_tab_features = local_tab_explanation.get_ranked_local_names()\n",
    "local_tab_importance = local_tab_explanation.get_ranked_local_values()\n",
    "\n",
    "\n",
    "# PFIExplainer\n",
    "from interpret.ext.blackbox import PFIExplainer\n",
    "\n",
    "pfi_explainer = PFIExplainer(\n",
    "    model=loan_model,\n",
    "    features = ['loan_amount','income','age','marital_status'],\n",
    "    classes=['reject','approve']\n",
    ")\n",
    "# global feature importance\n",
    "# The code is the same for MimicExplainer and TabularExplainer. The PFIExplainer requires the actual labels that correspond to the test features.\n",
    "global_pfi_explanation = pfi_explainer.explain_global(X_train, y_train)\n",
    "global_pfi_feature_importance = global_pfi_explanation.get_feature_importance_dict()\n",
    "\n",
    "# The PFIExplainer doesn't support local feature importance explanations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Â Creating explanations\n",
    "When you create an estimator/script in an AML Experiment, you can create an explainer that uploads its explanation to the run for later analysis.\n",
    "\n",
    "### Creating an explanation in the experiment script\n",
    "you'll need to have the following packages:\n",
    "- azureml-interpret\n",
    "- azureml-contrib-interpret\n",
    "make sure you have these installed in your run environment\n",
    "\n",
    "example code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.run import Run \n",
    "from azureml.contrib.interpret.explanation.explanation_client import ExplanationClient \n",
    "from interpret.ext.blackbox import TabularExplainer \n",
    "\n",
    "run = Run.get_context()\n",
    "\n",
    "# code to train mode goes here.... \n",
    "\n",
    "# get explanations - tabularexplainer based on SHAP\n",
    "explainer = TabularExplainer(\n",
    "    model,\n",
    "    X_train,\n",
    "    features=features,\n",
    "    classes=labels\n",
    ")\n",
    "explanation = explainer.explain_global(X_test)\n",
    "\n",
    "# get explanation client and upload the explanations\n",
    "explain_client = ExplanationClient.from_run(run)\n",
    "explain_client.upload_model_explanation(\n",
    "    explanation,\n",
    "    comment = 'Tabular Explanation'\n",
    ")\n",
    "\n",
    "run.complete()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can view explanations in AML Studio, or use the **ExplanationClient** object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.contrib.interpret.explanation.explanation_client import ExplanationClient \n",
    "\n",
    "client = ExplanationClient.from_run_id(\n",
    "    workspace=ws,\n",
    "    experiment_name=experiment.experiment_name,\n",
    "    run_id=run.id\n",
    ")\n",
    "explanation = client.download_model_explanation()\n",
    "feature_importances = explanation.get_feature_importance_dict()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}