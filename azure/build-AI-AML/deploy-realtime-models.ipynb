{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deploying ML models with AML\n",
    "\n",
    "This notebook will discuss how we will deploy models in AML.\n",
    "\n",
    "In ML, we normally use a trained model to predict labels for new data in which our model has not been trained, this is called `inferencing`. A model is often deployed to allow apps to request real-time predictions for single/small amount of new data observations.\n",
    "\n",
    "In AML, we create real-time inferencing solutions by deploying a trained model as a service, and hosting it in a containerised platform such as AKS."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploying a model for real-time inferencing\n",
    "\n",
    "You can deploy a trained model to a compute:\n",
    "- local compute\n",
    "- AML compute instance\n",
    "- Azure Container Instance (ACI)\n",
    "- Azure Kubernetes Service (AKS) cluster\n",
    "- Azure function\n",
    "- IoT module\n",
    "\n",
    "AML uses containers for deployments, this packages the model and the code that enables the models use, in an image that can be deployed to a container of your chosen compute. \n",
    "\n",
    "For testing, it is good to deploy to:\n",
    "- local compute\n",
    "- AML Computer Instance\n",
    "- ACI\n",
    "\n",
    "For production:\n",
    "- deploy to a target that meets the specific performance, scalability and security needs of your app architecture.\n",
    "\n",
    "To deploy a model for real-time consumption, you have to:  \n",
    "\n",
    "1. Register a trained model  \n",
    "2. Define an inference configuration  \n",
    "3. Define a deployment configuration  \n",
    "4. Deploy the model  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Register a Trained Model\n",
    "\n",
    "To register a model from a local file, you can use:\n",
    "- `register` method of the **Model** object\n",
    "- 'register_model' method of the **Run** object\n",
    "\n",
    "#### Example code\n",
    "option 1 - registering via the Model object\n",
    "\n",
    "```python\n",
    "azureml.core import Model\n",
    "\n",
    "model = Model.register(\n",
    "    workspace = ws,\n",
    "    model_name='some classification model',\n",
    "    model_path='model.pkl', # path to where you have stored the model locally\n",
    "    description='Classify something'\n",
    ")\n",
    "```\n",
    "\n",
    "option 2 - register model using Run object\n",
    "\n",
    "```python\n",
    "run.register_model(\n",
    "    model_name='classification model,\n",
    "    model_path='outputs/model.pkl',\n",
    "    description='It classifies something'\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Define an Inference Configuration\n",
    "\n",
    "To deploy the model, it will need to be deployed with:  \n",
    "\n",
    "1. InferenceConfig file: combines environment config file and entry script\n",
    "\n",
    "    - environment config: An environment in which the script is run  \n",
    "    - entry/scoring script: A scipt that loads the model and returns the predictions for some input data\n",
    "\n",
    "\n",
    "These two, `entry/scoring script` and the `environment config` are both combined in an `InferenceConfig` file.\n",
    "\n",
    "#### 2.1 Entry/Scoring Script  \n",
    "Lets create the script that runs the model and returns the prediction\n",
    "\n",
    "This script must contain 2 functions:\n",
    "- init(): \n",
    "    - called when the service is initialised\n",
    "    - loads model from model registery\n",
    "- run(raw_data): \n",
    "    - called when new data is submitted to the service\n",
    "    - run function to generate prediction \n",
    "\n",
    "Example:\n",
    "```python\n",
    "import json\n",
    "import joblib\n",
    "import numpy as np \n",
    "import azureml.core.model import Model \n",
    "\n",
    "# Init() - This will load the model from the model registry\n",
    "def init():\n",
    "    global model\n",
    "    #Â get path to registered model file and load it\n",
    "    model_path = Model.get_model_path('classification_model')\n",
    "    model = joblib.load(model_path)\n",
    "\n",
    "# run(raw_data) - called when a request is received\n",
    "def run(raw_data):\n",
    "    # convert to numpy array\n",
    "    data = np.array(json.loads(raw_data)['data'])\n",
    "    # get prediction from model\n",
    "    predictions = model.predict(data)\n",
    "    # return prediction as JSON serialisable format\n",
    "    return predictions.tolist()\n",
    "```\n",
    "\n",
    "#### 2.2 Creating & Defining Environment\n",
    "\n",
    "For the model to be called and run, you need a python environment in which to run the python entry script above. \n",
    "\n",
    "You can configure this environment using a conda config file - created using CondaDependencies class. \n",
    "\n",
    "Example:\n",
    "\n",
    "```python\n",
    "from azureml.core.conda_dependencies import CondaDependencies\n",
    "\n",
    "# add dependencies model needs\n",
    "myenv = CondaDependencies()\n",
    "myenv.add_conda_package('scikit-learn')\n",
    "\n",
    "# save environment config as a .yml file\n",
    "env_file = 'service_files/env.yml'\n",
    "with open(env_file, 'w') as f:\n",
    "    f.write(myenv.serialisable_to_string())\n",
    "print('saved dependency info')\n",
    "```\n",
    "\n",
    "#### 2.3 Combining in a InferenceConfig file\n",
    "\n",
    "Here, you will combine the entry script and the environment config file.\n",
    "\n",
    "example:\n",
    "```python\n",
    "from azureml.core.model import InferenceConfig\n",
    "\n",
    "classifier_inference_config = InferenceConfig(\n",
    "    runtime='python',\n",
    "    source_directory='service_files',\n",
    "    entry_script='score.py',\n",
    "    conda_file='env.yml'\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Define a Deployment Config\n",
    "\n",
    "Now, where will the script and its dependency file be deployed to? You need some compute!\n",
    "\n",
    "Here is an example: deploying to `AKS`\n",
    "Remember, for deploying to AKS, you need to define the compute and the cluster:\n",
    "\n",
    "1. Create compute target\n",
    "```python\n",
    "from azureml.core.compute import ComputeTarget, AksCompute\n",
    "\n",
    "cluster_name='aks-cluster'\n",
    "compute_config = AksCompute.provisioning_configuration(location='westeu')\n",
    "production_cluster = ComputeTarget.create(ws, cluster_name, compute_config)\n",
    "```\n",
    "\n",
    "2. define deployment config\n",
    "We will set the target-specific compute spec we need for container deployment\n",
    "```python\n",
    "from azureml.core.webservice import AksWebservice\n",
    "\n",
    "classifier_deploy_config = AksWebservice.deploy_configuration(\n",
    "    cpu_cores=1,\n",
    "    memory_gb=1\n",
    ")\n",
    "```\n",
    "\n",
    "Above, we defined the steps for an AKS (cluster and compute). But for deploying to ACI:\n",
    "- you do not need to create an ACI compute target\n",
    "- use `deploy_configuration` class from `azureml.core.webservice.AciWebservice`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Deploy the Model\n",
    "\n",
    "Now we just need to deploy it!\n",
    "\n",
    "To do this, call `deploy` on the `Model` class.\n",
    "\n",
    "```python\n",
    "from azureml.core.model import Model\n",
    "\n",
    "model = ws.models['classification_model']\n",
    "\n",
    "service = Model.deploy(\n",
    "    workspace=ws,\n",
    "    models=[model],\n",
    "    inference_config= classifier_inference_config,\n",
    "    deployment_config= classifier_deploy_config,\n",
    "    deployment_target=production_cluster\n",
    ")\n",
    "\n",
    "service.wait_for_deployment(show_output = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Consuming Real-time inferencing service\n",
    "\n",
    "You can consume via a REST endpoint or via the aml sdk.\n",
    "\n",
    "### Using AML SDK\n",
    "\n",
    "- This is mostly for testing \n",
    "- call web service using the run method of a WebService object that references the deployed service\n",
    "\n",
    "Example:\n",
    "```python\n",
    "import json\n",
    "\n",
    "x_new = [[0.1,2.3,4.1,2.0],[0.2,1.8,3.9,2.1]]\n",
    "\n",
    "json_data = json.dumps({\n",
    "    'data':x_new\n",
    "})\n",
    "\n",
    "predictions = json.loads(response)\n",
    "\n",
    "for i in range(len(x_new)):\n",
    "    print(x_new[i], predictions[i])\n",
    "```\n",
    "\n",
    "### Using Rest endpoint\n",
    "\n",
    "in prod, most apps wont use AML SDK. instead, it will use the REST endpoint.\n",
    "\n",
    "How to determine the REST endpoint:\n",
    "- In AML Studio\n",
    "- using the `scoring_uri` variable of WebService object\n",
    "\n",
    "example:\n",
    "```python\n",
    "endpoint = service.scoring_uri\n",
    "print(endpoint)\n",
    "```\n",
    "\n",
    "What about authentication?\n",
    "You may want to restrict access to the endpoint by apps by using authentication. You can either use:\n",
    "- Key: requests are authenticated by specifying a key\n",
    "- Token: requests are authenticated by providing a JSON web token (JWT)\n",
    "\n",
    "ACI disables authentication & AKS is set to key-based.\n",
    "\n",
    "if you have an authenticated session with the ws, you can retreive the keys for a service by using **get_keys** method of the **WebService** obj.\n",
    "\n",
    "For token-based auth, you have to use service-principle auth for your client app. This verifies the identitiy through Azure AD and will call **get_token**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Troubleshooting service deployment\n",
    "\n",
    "A real-time deployment has a lot of moving parts:\n",
    "- trained model\n",
    "- runtime environment config\n",
    "- scoring script\n",
    "- container image\n",
    "- container host\n",
    "\n",
    "But how can we troubleshoot this! \n",
    "There are a few options:\n",
    "- check the service state\n",
    "- review service logs\n",
    "- deploy to a local container \n",
    "\n",
    "### 1. Check the service state\n",
    "you can check the status of a service by checking the **state**. \n",
    "\n",
    "to view the state, you must use the compute-specific service type (not the generic WebService, but instead something like **Aks**Webservice)\n",
    "\n",
    "Here is an example:\n",
    "```python\n",
    "from azureml.core.webservice import AksWebservice\n",
    "\n",
    "service = AciWebservice(name='classifier-service', workspace=ws)\n",
    "print(service.state)\n",
    "```\n",
    "\n",
    "If you get a ***Healthy*** back, you're all set!\n",
    "\n",
    "### 2. Review Service logs\n",
    "call the **get_logs()** method of a service object. The logs include detailed info about the provisioning of the service.\n",
    "\n",
    "### 3. Deploy to local container\n",
    "deployment and runtime errors are easier to diagnose on a local container, as you can make changes to scoring files that is referenced in the inference config, and reload the service without redploying it. You can only do this in the local container/deployment."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}