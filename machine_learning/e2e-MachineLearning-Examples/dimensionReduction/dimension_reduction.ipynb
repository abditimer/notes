{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Dimension Reduction\n",
    "\n",
    "We tend to have very large datasets for typical ML problems. In these problems, it is usual to have millions of features for each training instance. This can lead to extremely slow training times and can make it more difficult to find a good solution.\n",
    "\n",
    "This is referred to as the `Curse of Dimensionality`.\n",
    "\n",
    "Reducing dimensions does mean that we lose some information, which may make the performance worse. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "There are two main approaches to dimensionality reduction:\n",
    "* projection\n",
    "* Manifold learning"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## What is projection\n",
    "* in most real datasets, instances are not spread out uniformly across all dimensions\n",
    "* some features are constant and some are highly correlated\n",
    "* most lie on a much lower-dimensional subspace from the high-dimensional space \n",
    "* hence, can we project our data points onto a new plane?\n",
    "\n",
    "Here is an example:\n",
    "\n",
    "![pca image](https://s3.amazonaws.com/files.dezyre.com/images/Tutorials/Principal+Component+Analysis.jpg)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# Manifold Learning\n",
    "Manifolds can be thought of as rolls in higher dimensional space. For example, a 2D manifold is a 2D shape that can be bent and twisted in a higher dimensional space - but the shape has a 2D shape (e.g. a swiss roll - this is a famous example for manifolds). \n",
    "\n",
    "You can think of it as: a *d* dimensional manifold is a part of a n-dimneionsal space (where d<n), that locally resembles a d-dimensional hyperplane.\n",
    "\n",
    "Many dimensionality reducing algorithms work by modeling the manifolds on which the training instances lie - this is called manifold learning. \n",
    "\n",
    "This however is under the manifold assumption/hypothesis, which states that most real world high dimensional datasets lie close to a much lower dimensional manifold. \n",
    "\n",
    "There is also an additional assumption, which states that with the task you are trying to build a solution for (classification or regression), this solution will be simpler if it is expressed in a lower dimensional space of the manifold. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}